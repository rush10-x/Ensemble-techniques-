{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **THEORY QUESTIONS**"
      ],
      "metadata": {
        "id": "FEZne7D9D_cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Can we use Bagging for regression problems?\n",
        "\n",
        "- Yes, Bagging can be used for regression problems.\n",
        "Bagging, or Bootstrap Aggregating, is an ensemble method that can be applied to both classification and regression tasks.\n",
        "\n",
        " In regression, Bagging works by training multiple models (often decision trees) on different subsets of the training data and then averaging their predictions to improve accuracy and reduce variance.\n",
        "\n",
        "Q2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "- Multiple Model Training:\n",
        "\n",
        " Involves training several models independently on the same dataset or different subsets of the dataset.\n",
        " Aims to improve performance by combining the predictions of multiple models (e.g., through averaging or voting).\n",
        " Reduces the risk of overfitting and increases robustness.\n",
        "\n",
        "- Single Model Training:\n",
        "\n",
        " Involves training one model on the entire dataset.\n",
        "\n",
        " Simpler and faster but may lead to overfitting, especially with complex models.\n",
        "\n",
        " Performance is solely dependent on the single model's ability to generalize from the training data.\n",
        "\n",
        "Q3. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "- Feature Randomness:\n",
        "\n",
        " In Random Forest, feature randomness refers to the practice of selecting a random subset of features for each decision tree during the training process.\n",
        "\n",
        " This randomness helps to ensure that the trees are diverse and reduces the correlation between them, which enhances the overall model's performance.\n",
        "\n",
        " By using different subsets of features, Random Forest can capture a wider range of patterns in the data, leading to better generalization and reduced overfitting.\n",
        "\n",
        "Q4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "- Definition:\n",
        "\n",
        " The Out-of-Bag (OOB) Score is a validation technique used in Random Forest models to estimate the model's performance without needing a separate validation dataset.\n",
        "\n",
        "- How it Works:\n",
        "\n",
        " During the training of each decision tree, some samples are not included in the bootstrap sample (the training set for that tree). These samples are referred to as \"out-of-bag\" samples.\n",
        "\n",
        " The OOB Score is calculated by using these out-of-bag samples to evaluate the model's predictions, providing an unbiased estimate of the model's accuracy.\n",
        "\n",
        "- Advantages:\n",
        "\n",
        " No data leakage, as the OOB samples are not used in training the trees.\n",
        "\n",
        " Provides a reliable estimate of model performance, especially useful for small to medium-sized datasets.\n",
        "\n",
        "Q5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "- Feature Importance Measurement:\n",
        "\n",
        " Random Forest provides several methods to measure the importance of features:\n",
        "\n",
        "- Mean Decrease Impurity (MDI):\n",
        "\n",
        " Measures the total decrease in node impurity (e.g., Gini impurity or entropy) brought by a feature across all trees in the forest.\n",
        "\n",
        " Features that lead to larger decreases in impurity are considered more important.\n",
        "\n",
        "- Mean Decrease Accuracy (MDA):\n",
        "\n",
        " Evaluates the impact of permuting a feature on the model's accuracy.\n",
        "If permuting a feature significantly decreases the model's accuracy, that feature is deemed important.\n",
        "- Permutation Importance:\n",
        "\n",
        " Involves shuffling the values of a feature and measuring the change in model performance.\n",
        "\n",
        " A significant drop in performance indicates that the feature is important for the model's predictions"
      ],
      "metadata": {
        "id": "FEHrqR7MEHZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the working principle of a Bagging Classifier?\n",
        "\n",
        "- Definition: Bagging, or Bootstrap Aggregating, is an ensemble learning technique that aims to improve the stability and accuracy of machine learning algorithms.\n",
        "\n",
        " Working Steps:\n",
        "\n",
        " Data Sampling: Create multiple subsets of the training dataset using bootstrap sampling (random sampling with replacement). Each subset may contain duplicate instances.\n",
        "\n",
        " Model Training: Train a separate model (often a weak learner like a decision tree) on each subset of the data independently.\n",
        "\n",
        " Prediction Aggregation: For classification tasks, the final prediction is made by majority voting among the predictions of all models. For regression tasks, the predictions are averaged.\n",
        "\n",
        " Goal: The primary goal of bagging is to reduce variance and prevent overfitting by averaging the predictions of multiple models.\n",
        "\n",
        "Q7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "\n",
        "- Common Evaluation Metrics:\n",
        "\n",
        " Accuracy: The proportion of correct predictions made by the model.\n",
        "\n",
        " Precision: The ratio of true positive predictions to the total predicted positives.\n",
        "\n",
        " Recall (Sensitivity): The ratio of true positive predictions to the total actual positives.\n",
        "\n",
        " F1 Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
        "\n",
        " ROC-AUC: The area under the Receiver Operating Characteristic curve, which evaluates the trade-off between true positive rate and false positive rate.\n",
        "\n",
        " Cross-Validation: Use k-fold cross-validation to assess the model's performance on different subsets of the data, ensuring that the evaluation is robust and not dependent on a single train-test split.\n",
        "\n",
        " Confusion Matrix: Analyze the confusion matrix to understand the types of errors made by the classifier.\n",
        "\n",
        "Q8. How does a Bagging Regressor work?\n",
        "\n",
        "- Definition: A Bagging Regressor is similar to a Bagging Classifier but is used for regression tasks.\n",
        "\n",
        " Working Steps:\n",
        "\n",
        " Data Sampling: Generate multiple bootstrap samples from the original dataset.\n",
        "\n",
        " Model Training: Train a separate regression model (e.g., decision tree regressor) on each bootstrap sample independently.\n",
        "\n",
        " Prediction Aggregation: Combine the predictions from all models by averaging them to produce the final output.\n",
        "\n",
        " Goal: The main objective is to reduce variance in the predictions, leading to a more stable and accurate regression model.\n",
        "\n",
        "Q9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "- Improved Accuracy: Ensemble techniques often lead to better predictive performance than individual models by combining their strengths.\n",
        "\n",
        " Reduction of Overfitting: Techniques like bagging help reduce overfitting by averaging predictions from multiple models, which stabilizes the output.\n",
        "\n",
        " Robustness: Ensemble methods are generally more robust to noise and outliers in the data, as they leverage multiple models to make predictions.\n",
        "\n",
        " Flexibility: They can be applied to various types of models and problems, making them versatile in different machine learning scenarios.\n",
        "\n",
        "Q10. What is the main challenge of ensemble methods?\n",
        "\n",
        "- Increased Complexity: Ensemble methods can be more complex to implement and interpret compared to single models, making them harder to debug and understand.\n",
        "\n",
        " Computational Cost: Training multiple models can be computationally expensive and time-consuming, especially with large datasets.\n",
        "\n",
        " Risk of Overfitting: While ensemble methods reduce overfitting, they can still overfit if the base models are too complex or if the ensemble is not properly tuned.\n",
        "\n",
        " Dependency on Base Models: The performance of ensemble methods heavily relies on the choice of base models. Poorly chosen models can lead to suboptimal performance."
      ],
      "metadata": {
        "id": "zdxQE5TPF84b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Explain the key idea behind ensemble techniques?\n",
        "\n",
        "- Definition: Ensemble techniques combine multiple models (classifiers or regressors) to improve overall performance compared to individual models.\n",
        "\n",
        " Key Ideas:\n",
        "\n",
        " Diversity: By using different models or training on different subsets of data, ensemble methods can capture a wider range of patterns.\n",
        "\n",
        " Reduction of Overfitting: Combining predictions helps to mitigate the risk of overfitting that individual models may suffer from.\n",
        "\n",
        " Improved Accuracy: The final prediction is often more accurate as it averages out the errors of individual models.\n",
        "\n",
        "Q12. What is a Random Forest Classifier?\n",
        "\n",
        "- Definition: A Random Forest Classifier is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of their predictions for classification tasks.\n",
        "\n",
        " Key Features:\n",
        "\n",
        " Bagging Technique: It uses bootstrapping (sampling with replacement) to create multiple subsets of the training data.\n",
        "\n",
        " Random Feature Selection: At each split in the decision trees, a random subset of features is considered, which enhances diversity among the trees.\n",
        "\n",
        " Final Prediction: For classification, the final output is determined by majority voting among the trees.\n",
        "\n",
        "Q13. What are the main types of ensemble techniques?\n",
        "\n",
        "- Bagging:\n",
        "\n",
        " Combines predictions from multiple models trained on different subsets of the data.\n",
        "\n",
        " Example: Random Forest.\n",
        "\n",
        "- Boosting:\n",
        "\n",
        " Sequentially builds models, where each new model focuses on correcting errors made by the previous ones.\n",
        "\n",
        " Example: AdaBoost, Gradient Boosting.\n",
        "- Stacking:\n",
        "\n",
        "\n",
        "Combines multiple models (base learners) and uses their predictions as input for a higher-level model (meta-learner).\n",
        "- Voting:\n",
        "\n",
        " Aggregates predictions from multiple models, either by majority voting (for classification) or averaging (for regression).\n",
        "\n",
        "Q14. What is ensemble learning in machine learning?\n",
        "\n",
        "- Definition: Ensemble learning is a machine learning paradigm where multiple models are trained to solve the same problem and their predictions are combined to produce a more accurate and robust final prediction.\n",
        "\n",
        " Purpose:\n",
        "\n",
        " To improve model performance by leveraging the strengths of various algorithms.\n",
        "\n",
        " To reduce the likelihood of poor predictions by averaging out errors.\n",
        "\n",
        " Applications: Used in various domains such as finance, healthcare, and image recognition to enhance predictive accuracy.\n",
        "\n",
        "Q15. When should we avoid using ensemble methods?\n",
        "\n",
        "- Small Datasets:\n",
        "\n",
        " When the dataset is small, simpler models may perform better due to lower complexity and reduced risk of overfitting.\n",
        "\n",
        " Real-Time Predictions:\n",
        "\n",
        " Ensemble methods, especially those with many trees, can be computationally expensive and slow, making them unsuitable for applications requiring real-time predictions.\n",
        "\n",
        " Limited Resources:\n",
        "\n",
        " In environments with limited computational resources, the overhead of training multiple models may not be feasible.\n",
        "\n",
        " High Dimensionality:\n",
        "\n",
        " In cases with a very high number of features, ensemble methods may not provide significant benefits and could lead to increased complexity without improved performance."
      ],
      "metadata": {
        "id": "8jBzDKCqG92D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "- Variance Reduction:\n",
        "\n",
        " Bagging (Bootstrap Aggregating) reduces the variance of the model by averaging the predictions of multiple models trained on different subsets of the data.\n",
        "\n",
        " Diversity Among Models:\n",
        "\n",
        " By training each model on a different bootstrap sample (random samples with replacement), Bagging introduces diversity among the base learners, which helps in generalizing better to unseen data.\n",
        "\n",
        " Stability:\n",
        "\n",
        " The aggregation of predictions from multiple models leads to a more stable and robust final prediction, which is less likely to overfit to the noise in the training data.\n",
        "\n",
        " Independence of Models:\n",
        "\n",
        " Each model is trained independently, which means that the errors made by individual models are less likely to be correlated, further reducing the risk of overfitting.\n",
        "\n",
        "Q17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "- Reduction of Overfitting:\n",
        "\n",
        " Random Forest mitigates the overfitting problem commonly associated with single decision trees by averaging the results of multiple trees, which smooths out the predictions.\n",
        "\n",
        "- Feature Randomness:\n",
        "\n",
        " At each split in the decision trees, Random Forest selects a random subset of features, which adds an additional layer of randomness and helps in capturing more complex patterns in the data.\n",
        "\n",
        "- Improved Accuracy:\n",
        "\n",
        " The ensemble approach of combining multiple trees generally leads to better accuracy compared to a single decision tree, especially in complex datasets.\n",
        "- Robustness:\n",
        "\n",
        " Random Forest is less sensitive to noise and outliers in the data, making it a more robust model for various applications.\n",
        "\n",
        "Q18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "- Data Subsets Creation:\n",
        "\n",
        " Bootstrap sampling involves creating multiple subsets of the training data by sampling with replacement. This means that some instances may appear multiple times in a subset, while others may not appear at all.\n",
        "\n",
        " Independence of Models:\n",
        "\n",
        " Each model in Bagging is trained on a different bootstrap sample, which ensures that the models are independent of each other, leading to diverse predictions.\n",
        "\n",
        " Variance Reduction:\n",
        "\n",
        " By training on different subsets, bootstrap sampling helps in reducing the overall variance of the model, which is crucial for improving generalization to unseen data.\n",
        "\n",
        " Aggregation of Predictions:\n",
        "\n",
        " The final prediction is made by aggregating the predictions from all models trained on the bootstrap samples, which enhances the stability and accuracy of the model.\n",
        "\n",
        "Q19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "- Finance:\n",
        "\n",
        " Credit scoring and risk assessment models often use ensemble techniques to improve prediction accuracy.\n",
        "\n",
        " Healthcare:\n",
        "\n",
        " Ensemble methods are used in disease prediction and diagnosis, where combining multiple models can lead to better outcomes.\n",
        "\n",
        " Image Recognition:\n",
        "\n",
        " In computer vision, ensemble techniques enhance the performance of models in tasks like object detection and image classification.\n",
        "\n",
        " Natural Language Processing:\n",
        "\n",
        " Sentiment analysis and text classification benefit from ensemble methods to improve accuracy and robustness.\n",
        "\n",
        " Fraud Detection:\n",
        "\n",
        " Ensemble techniques are employed in detecting fraudulent transactions by combining multiple models to identify patterns indicative of fraud.\n",
        "\n",
        "Q20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "- Methodology:\n",
        "\n",
        " Bagging: Builds multiple models independently and combines their predictions (e.g., averaging or voting).\n",
        "\n",
        " Boosting: Builds models sequentially, where each new model focuses on correcting the errors made by the previous ones.\n",
        "\n",
        " Model Independence:\n",
        "\n",
        " Bagging: Models are trained independently, which helps in reducing variance.\n",
        "\n",
        " Boosting: Models are dependent on each other, as each model is trained based on the performance of the previous ones.\n",
        "\n",
        " Error Handling:\n",
        "\n",
        " Bagging: Reduces variance by averaging predictions from multiple models.\n",
        "\n",
        " Boosting: Reduces bias by focusing on difficult-to-predict instances and adjusting the weights of misclassified examples.\n",
        "\n",
        " Performance:\n",
        "\n",
        " Bagging: Generally performs better with high-variance models (like decision trees).\n",
        "\n",
        " Boosting: Often yields better performance on complex datasets by reducing bias and improving accuracy"
      ],
      "metadata": {
        "id": "Bql9bjdDI_dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "pX7icsI_KER1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "\n",
        "# Create Bagging Classifier\n",
        "model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "8CKOwmq7KJma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_estimator = DecisionTreeRegressor()\n",
        "\n",
        "# Create Bagging Regressor\n",
        "model = BaggingRegressor(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print MSE\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "BSCfS8vRKK3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "wfONPV6pKMOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24.  Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Print MSE for both models\n",
        "print(\"Decision Tree MSE:\", dt_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "id": "-XsZwgSKKN-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data (not required for OOB but done for clarity)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest with OOB enabled\n",
        "model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB score\n",
        "print(\"OOB Score:\", model.oob_score_)\n"
      ],
      "metadata": {
        "id": "mI23BE7qKPJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_estimator = SVC(probability=True, kernel='rbf')\n",
        "\n",
        "# Create Bagging Classifier\n",
        "model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "vEDfboBpKQUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different number of trees\n",
        "n_estimators_list = [1, 10, 50, 100, 200]\n",
        "\n",
        "# Train and evaluate models\n",
        "for n in n_estimators_list:\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with {n} trees: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "8ygBna_1KRtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_estimator = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "\n",
        "# Create Bagging Classifier\n",
        "model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Print AUC score\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "id": "jYt6gfOeOcKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q29. Train a Random Forest Regressor and analyze feature importance scores\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "afy88dcUKYav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        " from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging with Decision Tree\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "# Compare accuracy\n",
        "print(\"Bagging Accuracy:\", bagging_acc)\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n"
      ],
      "metadata": {
        "id": "6lB_KHTnKZ1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 4],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "mcS-jkH3PFBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Test different numbers of base estimators\n",
        "estimators_range = [1, 5, 10, 20, 50, 100]\n",
        "mse_scores = []\n",
        "\n",
        "for n_estimators in estimators_range:\n",
        "    bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                n_estimators=n_estimators, random_state=42)\n",
        "    bagging.fit(X_train, y_train)\n",
        "    y_pred = bagging.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(estimators_range, mse_scores, marker='o')\n",
        "plt.xlabel('Number of Base Estimators')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Performance of Bagging Regressor with Varying Estimators')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VmWeeuJLpJGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q33. Train a Random Forest Classifier and analyze misclassified samples\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "misclassified_samples = X_test[misclassified_indices]\n",
        "actual_labels = y_test[misclassified_indices]\n",
        "predicted_labels = y_pred[misclassified_indices]\n",
        "\n",
        "# Display misclassified samples\n",
        "df_misclassified = pd.DataFrame(misclassified_samples, columns=feature_names)\n",
        "df_misclassified['Actual Label'] = [target_names[i] for i in actual_labels]\n",
        "df_misclassified['Predicted Label'] = [target_names[i] for i in predicted_labels]\n",
        "\n",
        "# Output results\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=target_names))\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(df_misclassified)\n"
      ],
      "metadata": {
        "id": "jSWxMkj9pKP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train single Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Train Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                            n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Print and compare performance\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "0JrIT4_7pLf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q35. Train a Random Forest Classifier and visualize the confusion matrix\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Compute and visualize confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uNO6qD9DpM7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42))\n",
        "]\n",
        "\n",
        "# Define final estimator\n",
        "final_estimator = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Build stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Compare base models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy scores\n",
        "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, dt.predict(X_test)):.4f}\")\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test, svm.predict(X_test)):.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr.predict(X_test)):.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "jpH4Zyo9pOSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q37. Train a Random Forest Classifier and print the top 5 most important features\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.fe\n"
      ],
      "metadata": {
        "id": "8LhwSuHZpPut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                n_estimators=50, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report (Precision, Recall, F1-score):\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "R1HdCPIopRTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q39.  Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Range of max_depth values to test\n",
        "max_depth_values = [1, 2, 3, 5, 10, None]\n",
        "accuracies = []\n",
        "\n",
        "# Train model with different max_depth values\n",
        "for depth in max_depth_values:\n",
        "    clf = RandomForestClassifier(max_depth=depth, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(8, 5))\n",
        "depth_labels = ['1', '2', '3', '5', '10', 'None']\n",
        "plt.plot(depth_labels, accuracies, marker='o')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of max_depth on Random Forest Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "viJ7uhCApTgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor with Decision Tree\n",
        "bagging_dt = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging Regressor with KNeighbors Regressor\n",
        "bagging_knn = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Print comparison\n",
        "print(f\"Mean Squared Error - Decision Tree Base Estimator: {mse_dt:.4f}\")\n",
        "print(f\"Mean Squared Error - KNeighbors Base Estimator:   {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "Mk82SO9-pUrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        " from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Binarize the output for multi-class ROC AUC\n",
        "y_binarized = label_binarize(y, classes=[0, 1, 2])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binarized, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = rf.predict_proba(X_test)\n",
        "\n",
        "# Calculate ROC-AUC score (macro average for multi-class)\n",
        "roc_auc = roc_auc_score(y_test, y_proba, average='macro', multi_class='ovr')\n",
        "\n",
        "# Print ROC-AUC score\n",
        "print(f\"ROC-AUC Score (Macro, Multi-class): {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "ECXNOYoN8f_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q42. Train a Bagging Classifier and evaluate its performance using cross-validation\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Initialize Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                n_estimators=50,\n",
        "                                random_state=42)\n",
        "\n",
        "# Evaluate using cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(cv_scores))\n",
        "print(\"Standard Deviation:\", np.std(cv_scores))\n",
        "\n"
      ],
      "metadata": {
        "id": "AqLsaicD8hmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q43. Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "y_binarized = label_binarize(y, classes=[0, 1, 2])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binarized, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = rf.predict_proba(X_test)\n",
        "\n",
        "# Plot Precision-Recall curve for each class\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(y_binarized.shape[1]):\n",
        "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_scores[i][:, 1])\n",
        "    ap_score = average_precision_score(y_test[:, i], y_scores[i][:, 1])\n",
        "    plt.plot(recall, precision, label=f'Class {i} (AP={ap_score:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Random Forest Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R9p5g4gr81i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        " from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Define final estimator\n",
        "final_estimator = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "stack_acc = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Train and evaluate individual models\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
        "\n",
        "# Print accuracy scores\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {stack_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "IVVp4JON82s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different bootstrap sample sizes (as a fraction of the training set)\n",
        "max_samples_list = [0.3, 0.5, 0.7, 0.9, 1.0]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different bootstrap sample sizes\n",
        "for max_samples in max_samples_list:\n",
        "    bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                n_estimators=50,\n",
        "                                max_samples=max_samples,\n",
        "                                bootstrap=True,\n",
        "                                random_state=42)\n",
        "    bagging.fit(X_train, y_train)\n",
        "    y_pred = bagging.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Plot the performance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(max_samples_list, mse_scores, marker='o')\n",
        "plt.xlabel('Bootstrap Sample Fraction (max_samples)')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Effect of Bootstrap Sample Size on Bagging Regressor Performance')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NvmjuphP84DZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}